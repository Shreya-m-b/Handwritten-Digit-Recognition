# -*- coding: utf-8 -*-
"""Handwritten_Digit_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ndO1Z5qpu1x5X0Zjb9xbzAXXeNzZ7iCF
"""

# Commented out IPython magic to ensure Python compatibility.
#importing all required libraries
import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
from keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,Flatten,Dropout,Dense,MaxPooling2D
from tensorflow.keras.optimizers import SGD
from keras.utils import np_utils 
# %matplotlib inline

#importing training and testing data
(train_X,train_y),(test_X,test_y) = mnist.load_data()
train_X.shape

#checking if we are able to get a visual of 28X28 images
plt.imshow(train_X[1],cmap='gray')

train_y

#reshaping the data as per keras 
train_X = train_X.reshape(-1,28,28,1)
test_X  = test_X.reshape(-1,28,28,1)
#convert integer to floats
train_X = train_X.astype('float32')
test_X  = test_X.astype('float32')
#normalising the grayscale image (2D-255) to the range 0-1
train_X = train_X/255
test_X  = test_X/255
#One Hot Coding the labels
train_y = np_utils.to_categorical(train_y)
test_y  = np_utils.to_categorical(test_y)

train_y[1]

#creating a sequential model and adding convolution layers,maxpooling layers,dropout layer and flatten the 2D results
model = Sequential()
model.add(Conv2D(32, kernel_size=(3,3), activation='relu',input_shape=(28,28,1), padding='SAME'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(64,(3,3),activation='relu',padding='SAME'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128,activation='relu'))
model.add(Dropout(0.5))
#use softmax activation as it gives highest probability to that neuron which will be at the index of the image provided.
model.add(Dense(10,activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=SGD(0.01), metrics=['accuracy'])
print(model.summary())

#training the data
batch_size=32
epochs=10
plotting_data = model.fit(train_X,
                          train_y,
                          batch_size=batch_size,
                          epochs=epochs,
                          verbose=1,
                          validation_data=(test_X,test_y))
loss,accuracy = model.evaluate(test_X,test_y,verbose=0)
print('Test loss ---> ',str(round(loss*100,2)) +str('%'))
print('Test accuracy ---> ',str(round(accuracy*100,2)) +str('%'))

#just for reference plot of test accuracy and training accuracy using an history attribute or an object which has loss ans accuracy values stored in it while training
plt.plot(epochs,test_accuracy,marker='X',label='test_accuracy')
plt.plot(epochs,training_accuracy,marker='X',label='training_accuracy')
plt.legend()